[
["index.html", "Introduction To Causality: A Modern Approach Welcome", " Introduction To Causality: A Modern Approach Florian Brezina Welcome This is the HTML version of “Introduction to Causality: A Moden Approach”, a gentle but rigorous introduction into the art and science of causal inference. This book covers the basics of causal inference: you will learn how causal inference differs from statistical inference or prediction; how to express these differences in unambiguous mathematical notation and causal graphs; a variety of techniques to probe causal questions, from randomized controlled experiments to structural equation models; the current scientific edge on causal analysis, including reinforcement learning. We approach this topic by closely examining most simple scenarios first and build upon those chapter by chapter. Throughout the book, we will use modern notation and language, primarily following Pearl (2000). The book further contains an extensive appendix containing code snippets in the statistical programming language R as well as auxilliary material on statistics. We hope that this will allow the book to be a good standalone source for all those interested in causality, whether or not they have a solid foundation in statistics. "],
["introduction.html", "1 Introduction 1.1 What you will learn 1.2 How this book is organised 1.3 What you won’t learn 1.4 Prerequisites 1.5 Acknowledgements 1.6 Links", " 1 Introduction Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there.’ — Randall Munroe Causal analysis is a fascinating field. It deals with the fundamental relation between cause and effect in complex environments. Being able to infer what the effect is going to be after doing A versus B is of utmost importance in a wide variety of applications, from policy analysis, drug prescription to marketing. Despite its ubiquity in all disciplines concerned with complex phenomena, the concept of causality has eluded a mathematically rigorous treatment for a long time, resulting in puzzling paradoxes and ambiguous statements. Only in recent decades a new formalism has emerged to solve these problems, with main contributors from the computer science and economics departments. The causal revolution[^footnote-causal-revolution] swept away decades of experts and students arguing about the correct interpretation of phenomena such as Simpson’s paradox, the nature and properties of the error term in regression equations and the interpretation of structural parameters in SEMs. The revolution established a new regime, which introduced new notation and a unified language, where causal and statistical concepts are finally separated. At last, correlation is never again to be confused with causation\". “Introduction To Causality” is a gentle introduction into this modern understanding of causality as it unfolded after the revolution. It will help you learn the fundamentals of this art and science of causal analysis. After reading this book, you’ll have the tools to understand and communicate causal concepts and you will know how to tackle the common questions. The code in the appendix will help you to apply these methods using the R programming language. 1.1 What you will learn First we will discuss causality in a trivial lab environment where we are able to control every important aspect of the environment. This will help us get familiar with the vocabulary and notation and will provide some insights in how to think about causality. Once we have become familiar with the simple setting, we will loosen the assumption that we are able to fully control the environment. At this point, we will introduce causality as a probabilistic concept. Our inability to fully control and understand our environment forces us to settle for a less precise inference on the effects. We can’t say what will happen, but we can still provide robust inference on what will happen on average. Most importantly, we will see why classical statistical concepts and notation are not sufficient for a rigorous and unambiguous treatment of causality and we will get a sense that there is a fundamental difference between what can be learned from passive observation (“correlation”) and active intervention (“causation”). Once we mastered the probabilistic nature of causality we will discuss on how we can measure the effect of actions in a variety of settings. We will start with the easiest scenario, the randomized controlled trial. It is often considered the gold standard for clinical trials and applied across scientific disciplines. It will serve as a benchmark in our further discussion, where we will look at scenarios that violate the assumption behind the randomized controlled trial: we will discuss observational studies, synthetic cohorts and time series analysis. After this tour de force, we shift gears and have a closer look at a couple of applications. We will discuss how to measure and interpret the placebo effect in clinical trials, how to optimize marketing using A/B tests and multi-armed bandits, and how to evaluate government interventions. Finally, I will wrap up this book by providing some parting thoughts on epistemology and the importance of causality in the evolution of artificial intelligence and machine learning. These chapters will hopefully provide you with a solid foundation and will allow you to find the right solution for your causal problem. But for most of your problems they will not be enough. Throughout this book we’ll point you to resources where you can learn more. The appendix provides some additional material on statistics and programming. 1.2 How this book is organised 1.3 What you won’t learn There are some important topics that this book doesn’t cover. We hope that this book will leave you wanting more and that you will continue in your journey to master causality by going deeper into this topic or by exploring closly related fields and applications that we did not cover in sufficient length. 1.3.1 Statistics The book focuses on causal inference rather than the statistics. Some basic statistical concepts are discussed in the appendix, but they primarily serve as a refresher and shows how to translate conventional statistical notation into a causal framework. We assume that the reader is (or has been) familiar with statistics as it is taught in most Statistics 101 classes. 1.3.2 Machine Learning We will address issues of machine learning where we see a connection to causal concepts. We do not go deep on any causal and non-causal ML algorithms. 1.3.3 Proofs The book does not contain any proof or any heavy mathematical derivations. 1.4 Prerequisites To get the most out of this book, you should be familiar with basic concepts of statistical analysis, nomeclature and notation. If “expected value”, “conditional probability” or “hypothesis test” are only vaguely familiar to you, please review the appendix before digging into the main text. The code snippets at the end of the book are purely optional. If you want to follow along on these, you need to have R on your computer. To download the software, go to CRAN, the comprehensive R archive n etwork. CRAN is composed of a set of mirror servers distributed around the world and is used to distribute R and R packages. Don’t try and pick a mirror that’s close to you: instead use the cloud mirror, https://cloud.r-project.org, which automatically figures it out for you. RStudio is an integrated development environment, or IDE, for R programming. Download and install it from http://www.rstudio.com/download. 1.5 Acknowledgements 1.6 Links A free HTML version of this book is available at https://flobrez.github.io/itc/. The markdown sources and supplementary material is available at https://github.com/flobrez/itc. "],
["defining-causality.html", "2 Defining Causality 2.1 Causality in simple environments 2.2 Causality in complex environments 2.3 Measuring causal effects", " 2 Defining Causality 2.1 Causality in simple environments Let’s first take a look at a maximally simple environment, shown in figure xxx. It represents a circuit diagram with a voltage source, a switch (X) and a lamp (Y). All elements of this environment can assume one of two states each, which we will conveniently encode as 0 and 1: * the switch can either be open (0) or closed (1) * the lamp can either be off (0) or on (1). Let’s further assume that the voltage source has enough capacity to lighten the lamp if the switch is closed. Although this system is easy to understand and reason about, let’s take an extra second to translate the circuit diagram into a causal graph, a representation that will become quite handy in more complex environments that will be discussed later in the book. Theorem 2.1 (Causal Graphs) A graph is a mathematical structure. It consists of a set of nodes and and a set of edges, where edges connect ordered pairs of nodes. In causal graphs, nodes represent variables; edges represent the causal relation from cause to effect. Note that in a causal graph, an edge is an ordered pair of nodes, the edge therefore directed. In most graphs in this book, we will consider causal systems that can be represeted as directed acyclical graphs (DAGs). We can further represent a causal graph as a set of structural equations. Due to its simplicity, the circuit diagram can be represented with a single equation: \\[\\begin{equation} Y := f(X) = X \\end{equation}\\] Note, that the equation uses operator “\\(:=\\)”\" rather than the usual “\\(=\\)”. It reads “\\(f(X)\\) is evaluated and assigned to \\(Y\\)” and therefore resembles variable assignment in many programming languages where, for example, x = x + 1 is a valid expression. Since the structural equation represents the causal mechanism relating the switch and the lamp, we can immediately read what happens if we intervene on the switch: when we close the switch, i.e. \\(do(X:= 1)\\), then the lamp will be on, \\(Y := 1\\); if we open the switch, \\(do(X:= 0)\\), then the lamp dies, \\(Y := 0\\). Let’s take it up a notch and create a more interesting environment by adding a second switch, connected in series, see figure XXX for the circuit diagram and figure xxx for the graph representation. The two switches allow the environment to be in four different states. Only if both switches are on, will the lamp be on, in the other three states it will be off: switch \\(X_1\\) switch \\(X_2\\) lamp \\(Y\\) 0 0 0 0 1 0 1 0 0 1 1 1 We can easily spot that the structural equation representation is \\[\\begin{equation} Y := f(X_1, X_2) = X_1 \\cdot X_2 \\end{equation}\\] Setzen wir in diesem System nun den Schalter 1 auf geschlossen, erkennen wir, dass der Zustand der Leuchte nun noch vom Zustand des zweiten Schalters abhängt. \\[\\begin{equation} Y := f(1, X_2) = 1 \\cdot X_2 = X_2 \\end{equation}\\] Nehmen wir nun an, dass wir nicht allwissend sind, sondern sich ein Schleier des Unwissens über Schalter 2 gelegt hat: wir wissen, dass es ihn gibt, wir wissen nur nicht, ob dieser geschlossen ist oder nicht. Man stellt uns nun die Aufgabe, den Zustand des Schalters 2 aus der rein passiven Beobachtung des Systems zu lernen. Ein Blick auf Tabelle xxx zeigt, dass uns das nur in einigen Fällen gelingen wird. Bestimmte Konstellationen von Schalter 1 und Leuchte geben uns nicht ausreichende Information, um den Zustand von Schalter 2 lernen zu können. Selbstverständlich könnten wir dieses Problem lösen, indem wir Schalter 1 betätigen und beobachten, wie sich Leuchte dann ändert. Tatsächlich können wir durch die Intervention und die weitere Beobachtung auf den Zustand von Schalter 2 zu schließen. Dies ist bereits ein erster Hinweis auf ein fundamentales Problem, auf das wir im Weiteren noch detaillierter eingehen werden: durch Interventionen in eine System können Informationen generiert werden, die aus rein passiver Beobachtung nicht verfügbar sind. Gehen wir nun noch einen Schritt weiter und überlegen wir uns, ob das Problem gelöst werden kann, indem wir 2.2 Causality in complex environments \\[\\begin{equation} \\Delta_i := Y_i^{S;do(Z_i:=1)} - Y_i^{S; do(Z_i:=0)} \\label{eq:myfirsteq} \\tag{1} \\end{equation}\\] As \\(Y\\) is binary, \\(\\Delta\\) can be one of \\({-1, 0, 1}\\) with \\(\\Delta = 1\\) being the desired outcome. As discussed in [causality], we are not able to measure this quantity directly, but need to resort to population-level quantities instead: \\[\\begin{equation} P(\\Delta) = P^{S;do(Z:=1)}(Y) - P^{S;do(Z:=0)}(Y) \\label{eq:mktg_pop_ate} \\tag{2} \\end{equation}\\] 2.3 Measuring causal effects The fundamental problem of causal inference The definition of [causal effect] hints at a severe problem for its measurements. It involves two quantities which can never be both observed at once. This poses a problem, a problem so fundamental that is often called the fundamental problem of causal inference. "],
["methods-for-causal-inference.html", "3 Methods for Causal Inference 3.1 Randomized Controlled Experiments 3.2 Instrumental Variables 3.3 Propensity Score Matching 3.4 Difference-in-Difference Estimator 3.5 Time Series Methods", " 3 Methods for Causal Inference Causal relations can be inferred from experiments as well as observational studies. The randomized controlled experiment is a proven method to infer causal relations in complex environments. It involves full control over the assignment mechanism and the assignment is random. A common variation is the situation where the variable of interest cannot be directly intervened on, but a causal parent can. For example, a doctor can (randomly) prescribe a certain drug, but the patient still chooses to take the drug or not. The method of instrumental variables allows us to infer (local) causal effects nevertheless. Afterwards, we switch to those methods that can be used even if we cannot intervene in the environment, but have to rely on passive observation only. Inferring causal relations in these situations requires a thorough understanding of the causal links from the variable of interest to the effect. We will study two different inference strategies which rely on different sets of assumptions. Finally, we discuss methods for causal inference in samples of size 1. Given appropriate assumptions, we are able to infer causal relations by leveraging (dependent) observations over time. 3.1 Randomized Controlled Experiments TODO 3.2 Instrumental Variables TODO 3.3 Propensity Score Matching TODO 3.4 Difference-in-Difference Estimator TODO 3.5 Time Series Methods TODO "],
["applications.html", "4 Applications 4.1 Marketing 4.2 Drug Trial 4.3 Discrimination", " 4 Applications 4.1 Marketing Another common application where correlation is often confused with causation is in marketing. This might be because data scientist and business might not speak the same language. It might, however, also be a valid pragmatic assumption, which is later validated in an experiment. I will focus here on the (mis-)application of propensity modeling. Propensity modeling attempts to predict if a (potential) customers will perform a certain action, e.g. whether a new visitor on your site will register or whether a customer will buy a certain product. The model output is an estimated probability that the customer will do the action. Propensity models therefore fall into the class of binary regression. \\[\\begin{equation} P(Y|I) = f(I) \\label{eq:mktg1} \\tag{1} \\end{equation}\\] where \\(I\\) is the information set available for prediction.1 There are plently of algorithms that could be used to estimate the function. Logistic regression is often chosen as it is easy to implement and the model itself might provide some insights. Here, we will focus not on the implementation part, but on the interpretation and (mis-) use of the model. To see why the model might not be want you think it is, we will have a A naive usage of the model is to focus marketing on customers with high likelhood to do the action. This however, can be serverly misleading, as we will discuss next. To show this, we will start with an ideal assumption: our model is perfect. A perfect model means that we predict the customer action correctly for every customer and the model produces predicted probabilies which are well calibrated. This means that we only get two predictions, either a customer will do action with estimated probability 0 or whether they will do so with probability 1 - and the model is always right. However, although we have the best possible model, it illustrates well why the naive interpreation cannot be correct. If we focus our marketing on those customers with highest propensity (as there are only two values it is those with predicted probability 1), we focus our attention on a customer group that buys the product anyways. In fact, these are the customers that we should least focus on as the best we can do is to have no effect on those customers (but we still have the cost of the marketing intervention, which might be an opportunity cost) and in some case we might even affect the customer adversely (as they might be annoyed by the marketing). Hence, we’re left with the second group of customers, those with predicted probability of 0. In this group, there might be some who will be convinced to buy the product after being exposed to the marketing, but we are not able to say which ones. Even after doing an experiment, we will not improve on our decision rule. Say, we run an A/B test on all customers who were predicted to not buy, and we estimate that 0.02 Depending on the situation, this model might not be very useful. The only thing that we learned from the model is that we should exclude those customers with highest probability from our marketing. This runs counter to our intuition. Furthermore, in many applications, the action might be a rare event, with likelihoods not much higher than 1%. Excluding these customers from marketing might not save a lot of money in the first place, and establishing a system where you’re able to provide different marketing on customer-level might have some fixed costs (e.g. it might require to store and process customer-level data and deploy the model-outputs to production systems). From the example it becomes clear that propensity modelling using a predictive model on passively observed data will at best be a proxy for the problem at hand. The goal of marketing optimization is to optimally trade-off the cost and effect of marketing, where the latter is a causal rather than an assosiative concept. Supervised machine learning models, regardless of their complexity, will fail at achieving the task since even a ideal falls short. To see this more clearly, let’s restate the problem in causal notation first. Let \\(Y\\) denote the binary customer action we want to predict. Further, let \\(S\\) denote the default environment, which includes all relevant causal factors that determine customer decisions, their preferences and endowment, the offers available on the market and our own offer and current marketing strategy. For simplicity’s sake, let’s assume our marketing strategy is binary and denote it by \\(M\\) (e.g. whether or not we send the customer a marketing email). Assume the current marketing strategy is \\(M = 0\\), i.e. we do currently not have an email program. The individual causal differential effect of sending an email to customer \\(i\\) is then \\[\\begin{equation} \\Delta_i := Y_i^{S;do(Z_i:=1)} - Y_i^{S; do(Z_i:=0)} \\label{eq:myfirsteq} \\tag{1} \\end{equation}\\] As \\(Y\\) is binary, \\(\\Delta\\) can be one of \\({-1, 0, 1}\\) with \\(\\Delta = 1\\) being the desired outcome. As discussed in [causality], we are not able to measure this quantity directly, but need to resort to population-level quantities instead: \\[\\begin{equation} P(\\Delta) = P^{S;do(Z:=1)}(Y) - P^{S;do(Z:=0)}(Y) \\label{eq:mktg_pop_ate} \\tag{2} \\end{equation}\\] Both quantities on the right-hand side of equation \\(\\eqref{eq:mktg_pop_ate}\\) can be estimated. There are couple of ways to do so, many of which we discussed in [causality]. The most straigtforward way is to apply a randomized controlled trial (or “A/B test”) where the population at hand is randomly split in two groups, one group being exposed to the marketing (i.e. \\(S;do(Z:=1)\\)) the other not being exposed (i.e. \\(S;do(Z:=0)\\)). Conditioning the probability estimate on a set of features allows us to investigate whether the differential causal effect is co-related with observable information - which ultimately tells us who will be most affected by the marketing. In the most simple case, we condition by a single discrete attribute \\(A\\), providing \\[\\begin{equation} P(\\Delta | A) = P^{S;do(Z:=1)}(Y | A) - P^{S;do(Z:=0)}(Y | A) \\label{eq:mktg_pop_cate} \\tag{3} \\end{equation}\\] This is superficially similar to equation \\(\\eqref{eq:mktg1}\\), but note that the right-hand side in $ describes two conditional probabilities drawn from two different environments. It will be helpful to rewrite this into a linear model form. Assume that \\(A\\) is binary. Then \\[\\begin{align} P(\\Delta | A = 1) &amp;= P^{S;do(Z:=1)}(Y | A = 1) - P^{S;do(Z:=0)}(Y | A = 1) \\\\ &amp;= a_1 - a_0 \\\\ &amp;=: \\Delta_1 \\\\ P(\\Delta | A = 0) &amp;= P^{S;do(Z:=1)}(Y | A = 0) - P^{S;do(Z:=0)}(Y | A = 0) \\\\ &amp;= a_3 - a_2 \\\\ &amp;=: \\Delta_0 \\end{align}\\] We can further represent \\(P(Y)\\) as a function of \\(Z\\) \\[\\begin{align} P(Y) &amp;= P^{S;do(Z:=0)}(Y) \\cdot (1-Z) + P^{S;do(Z:=1)}(Y) \\cdot Z \\\\ &amp;= P^{S;do(Z:=0)}(Y) + (P^{S;do(Z:=1)}(Y) - P^{S;do(Z:=0)}(Y)) \\cdot Z \\\\ &amp;= P^{S;do(Z:=0)}(Y) + P(\\Delta) \\cdot Z \\\\ &amp;= \\beta_0 + \\beta_1 Z \\end{align}\\] Further, replacing unconditional quantities with conditional ones, we can write \\[\\begin{align} P(Y | A) &amp;= P^{S;do(Z:=0)}(Y | A) + P(\\Delta | A) \\cdot Z \\\\ &amp;= \\alpha_0 + \\alpha_1 A + (\\Delta_0 + \\Delta_1 \\cdot A) \\cdot Z \\\\ &amp;= \\alpha_0 + \\alpha_1 A + \\Delta_0 Z + \\Delta_1 A Z \\end{align}\\] which looks quite familiar as it is the conventional way to specify a logistic regression equation on \\(A\\), \\(Z\\) and the interaction of both \\(A \\cdot Z\\).[^footnote-hte-nomenclature] If the treatment is ineffective \\(\\Delta_0 = 0\\) and \\(Delta_1 = 0\\). The differential causal effect is said to be heterogeneous, if \\(\\Delta_1 \\neq 0\\). [^footnote-hte-nomenclature]: The literature on heterogeneous treatment effect models often groups parameters of this equation regarding their role in application: \\(\\alpha_1\\) is called “prognostic” as it shows if and how the success rate differs across attributes \\(A\\) if no intervention/treatment is provided; \\(\\Delta_1\\) on the other hand is often called “predictive”, meaning how predictive \\(A\\) is on the effectiveness of the intervention/treatment, i.e. whether and by how much treatment effects differ across values of \\(A\\). The model can be generalized to the case where not just a single (binary) attribute is considered, but a vector of attributes. 4.2 Drug Trial 4.3 Discrimination In den letzten Jahren wurde in Politik, Medien und Wissenschaft häufiger über Diskriminierung diskutiert, einige Beispiele sind hierbei die Diskriminierung von Frauen im Berufsleben, der Wahl von Abegordneten zum Bundestag. Ein Verbot von Diskriminierung hat in Deutschland im Jahre 2006 Gesetzescharakter angenommen mit der Verabschiedung des Allgemeinen Gleichbehandlungsgesetzes (AGG), dass u.a. die Diskriminierung aus Gründen des Alters, der ethnischen Herkunft oder des Geschlechts unzulässig ist. Wie bereits des Gesetzesname andeutet, wird Diskriminierung als ein Vorgang angesehen, nicht als ein Zustand. Dies wird insebesondere in §3 deutlich, wo eine (unmittelbare) Diskriminierung dann vorliegt “wenn eine Person wegen [Alter, Herkunft oder Geschlecht] eine weniger günstige Behandlung erfährt, als eine andere Person in einer vergleichbaren Situation erfährt”. Die zunehmende Anwendung automatisierter Entscheidungsalgorithmen rückt die Frage nach Diskrimierung auch in den Blickpunkt der Forschung im Bereich des maschinellen Lernens. Es ist in diesem Kontext, dass dieses Thema eine mathematische Formalisierung erfahren hat. Dabei wurden zuletzt auch kausale Argumentationen und Notation eingeführt, siehe etwa Kilbertus etl al.. Wir werden uns im Folgenden an einem einfachen Beispiel diesem Problem nähern sowie einen Versuch der Die Identifikation von diskriminierenden Handlungen wird dadurch erschwert, dass die Handelnden Personen häufig nur einen Teil des gesamten Mechanismus kontrollieren und es begründete Sachzwänge gibt. Im Folgenden unterscheiden wir zwei Merkmalstypen First, let’s introduce some basic terminology first: * protected feature: this is a person’s feature that is legally protected, e.g. age or gender. * accepted feature: these are features that are explicitely or implicitely accepted for discrimination, e.g. the job might (by law) require a certain certificate of qualification or the job requires a certain skillset like a specific programming language or the ability to lift weigths heavier than 30 kg. While the protected features are usually unambiguous and explicitly stated in law, the accepted features can be reason for disagreement. It is usually the set of features that are deemed to be necessary to do the job, but of course “doing the job” can done in different qualities. A bar owner might believe it to be necessary for his waiters to be handsome and flirty with the predominantely female audience, but an applicant might think it is not. Getting the order right and providing the right servcie while being friendly he might consider to be sufficient. unprotected feature: these are all features that are not explicitly protected, e.g. eye color, ability to lift more than 30kg zulässige Merkmale: dies sind Merkmale, nach denen eine diskriminierende Handlung zulässig ist. So kann etwa für eine Tätigkeit in einem Warenlager voraussgesetzt werden, dass ein Bewerber in der Lage sein muss, Lasten über 20kg zu transportieren, da dies für die Erfüllung der Tätigkeit unabdingbar ist. aufhebende Merkmale: diese stellen eine Teilmenge der zulässigen Merkmale dar. Es sind diejenigen Merkmale, die auf dem kausalen Pfad zwischen den geschützten Merkmalen und dem Output liegen. Das Merkmal “kann Lasten über 20kg transportieren” ist vermutlich ein solches, da dieses kausal vom Geschlecht und/oder dem Alter eines Bewerbers beeinflusst ist. unzulässige Merkmale: sind Merkmale, nach denen eine diskriminierende Handlung nicht zulässig ist, die aber nicht ein geschütztes Merkmal sind. Dies könnte etwa die Augenfarbe des Bewerbers sein, die (etwa gemäß AGG nicht geschützt ist), jedoch für die Erfüllung der Tätigkeit irrelevant sein dürfte. stellvertretende Merkmale: diese stellen eine Teilmenge der unzulässigen Merkmale dar. Es sind diejenige Merkmale, die auf dem kausalen Pfad zwischen den geschützten Merkmalen und dem Output liegen. So kann die Augenfarbe eines Bewerbers ein stellvertretendes Merkmal sein, wenn die Augenfarbe etwa durch das Geschlecht oder die ethnische Herkunft beeinflusst wird. Sie werden stellvertretend genannt, da sie Information über das geschützte Merkmals beinhalten und somit zur Diskriminierung herangezogen werden könnten. TODO: sollte unzulässig nicht anders bezeichnet werden, da ja eine Diskriminierung nach nicht-stellvertretern rechtlich nicht verboten ist. Es gelten folgende Definitionen: kann ein Mechanismus der unmittelbaren Diskriminierung durch einen vom geschützten Merkmal unabhängigen Prozess (oder eine Konstante) ersetzt werden, ohne dass sich die Verteilung der Zielvariable ändert, so ist dieser Mechanismus nicht-diskriminierend Typically you will find a notation similar to \\(P(Y|X) = f(X)\\) where \\(X\\) is called a feature set or a vector/matrix of features. I use the term information set to deliberately distinguish between the notion of all the information you have available for a customer. The information set is abstract and represents information in potentially very different formats, e.g. order data in your data base, the recorded call with the customer service team, or the customer’s product reviews. Feature engineering is the step where this information is transformed into a format that can be used by ML algorithms: the order data could be transformed into multiple features, e.g. the total revenue in past 30 days, total revenue in past 180 days, the number of days the customer put an order in; the recorded call can transformed into days since the latest customer service contanct, length of the call, length of waiting in line and whether or not the issue was resolved; the product reviews are transformed into word embeddings.↩︎ "],
["epistemology.html", "5 Epistemology 5.1 Are all sciences equal?", " 5 Epistemology 5.1 Are all sciences equal? The concept of causality as discussed in this book is a cornerstone of many scientific disciplines. Although in most cases not stated explicitly in those terms, the question what will happen if i does x (versus y) is implicit in most scientific work in diverse fields such as economics, psychology, medicine, and history. The interventions contemplated and analyzed in these disciplines vary widely in terms of manipulability, scope and ethics: a randomized experiment on fiscal policy is neither operationally feasible nor ethically desirable - at least at a the level of entire nations; assessing how individuals provide different answers if questions are phrased differently is both manipulable in a controlled environment and too unintrusive to be ethically problematic. Throughout the book we discussed several methods to infer causal relationships ranging from randomized controlled trials to time series analysis. With every step away from the ideal properties of an RCT, the reliability and robustness of results crucially depended on the validity of the assumptions that we were willing to accept. In general, the more assumptions we make, the more likely the results will be unreliable. On a sidenote: of course, incorrect causal assumptions are not the only or even the main reason for scientific results to be unreliable. Many of those have to do with problems of statisical inference instead. Even in experiments following an RCT design, there are plenty of pitfalls to be aware of. Too small samples, p-hacking, and publishing bias are just a few reasons for the replication crisis in psychology, medicine and other social sciences. Least reliable results are to be expected when the complexity of the subject matter is very high, but the methodoligal toolkit to investigate it is reduced to the least reliable ones macroeconomics. growth, business cycle, monetary and fiscal policy. epidemiology. including nutrition. social psychology. evolution. while the theory of evolution has strong empirical support, combinatorial explosion and complex dynamics do not allow for a reliable inference of effects of interventions in the biosphere. Even if complexity is high, if some aspects of the problem can be thoroughly studied and combined using a consistent theory, the overall results can be reasonably reliable: climate science. microeconomics. pseudo-experiments on price controls (e.g. minimum wage or gas price cap) supported by strong theory of supply and demand. problem is often quantification of effects rather than the general direction. There is almost no disagreement across labor economists that there exists a general trade-off between higher minimum wages and lower unemployment. The debate today primarily focuses on the possibility of small negative or even positive effects on employment, but virtually everybody accepts the logic that unemployment will exist/increase above a certain threshold. "],
["notation-and-terminology.html", "A Notation and Terminology", " A Notation and Terminology Throughout the book we will follow notation and terminology similar to Peters, Janzik, and Schölkopf (2017). It deviates from Pearl (2009) primarily in the notation for intervention distributions. Where Pearl (2009) writes \\(P(Y|do(X = x))\\), we will use \\(P^{\\Gamma;do(X:=x)}(Y)\\) instead: it avoids confusion with the notation for conditional distributions and it emphasizes that an intervention is an assignment (“\\(:=\\)”) rather than an equality (“\\(=\\)”). Otherwise we follow conventional statistical notation. symbol represents \\(X, Y, U\\) (random) variable \\(x\\) value of \\(X\\) \\(X =x\\) \\(X\\) has value \\(x\\) \\(X:=x\\) assignment of value \\(x\\) to variable \\(X\\) \\(i\\) sample index, observation \\(\\Gamma\\) causal graph \\(\\Gamma;do(X:=x)\\) intervention on graph \\(\\Gamma\\) by assigning value \\(x\\) to \\(X\\) \\(P(Y)\\) probability distribution of \\(Y\\) \\(P(Y|X = x)\\) probability distribution of \\(Y\\) given \\(X = x\\) \\(P(Y|X)\\) set of probability distributions \\(P(Y|X = x) \\forall x\\) \\(P^{\\Gamma;do(X:=x)}(Y)\\) probability distributions of \\(Y\\) after intervention on graph \\(\\Gamma\\) \\(E(Y), \\mu_Y\\) expected value of \\(Y\\) \\(Var(Y), \\sigma_Y^2\\) variance of \\(Y\\) \\(\\hat{\\beta}\\) estimate of parameter \\(\\beta\\) \\(X -&gt; Y\\) causal link from \\(X\\) to \\(Y\\) References "],
["statistics-1.html", "B Statistics", " B Statistics TODO "],
["r-code.html", "C R Code", " C R Code TODO "]
]
