[
["index.html", "Introduction To Causality: A Modern Approach Welcome", " Introduction To Causality: A Modern Approach Florian Brezina Welcome This is the HTML version of “Introduction to Causality: A Moden Approach”, a gentle but rigorous introduction into the art and science of causal inference. This book covers the basics of causal inference: you will learn how causal inference differs from statistical inference or prediction; how to express these differences in unambiguous mathematical notation and causal graphs; a variety of techniques to probe causal questions, from randomized controlled experiments to structural equation models; the current scientific edge on causal analysis, including reinforcement learning. We approach this topic by closely examining most simple scenarios first and build upon those chapter by chapter. Throughout the book, we will use modern notation and language, primarily following Pearl (2000). The book further contains an extensive appendix containing code snippets in the statistical programming language R as well as auxilliary material on statistics. We hope that this will allow the book to be a good standalone source for all those interested in causality, whether or not they have a solid foundation in statistics. "],
["introduction.html", "1 Introduction 1.1 What you will learn 1.2 How this book is organised 1.3 What you won’t learn 1.4 Prerequisites 1.5 Acknowledgements 1.6 Links", " 1 Introduction Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there.’ — Randall Munroe Causal analysis is a fascinating field. It deals with the fundamental relation between cause and effect in complex environments. Being able to infer what the effect is going to be after doing A versus B is of utmost importance in a wide variety of applications, from policy analysis, drug prescription to marketing. Despite its ubiquity in all disciplines concerned with complex phenomena, the concept of causality has eluded a mathematically rigorous treatment for a long time, resulting in puzzling paradoxes and ambiguous statements. Only in recent decades a new formalism has emerged to solve these problems, with main contributors from the computer science and economics departments. The causal revolution (???) swept away decades of experts and students arguing about the correct interpretation of phenomena such as Simpson’s paradox, the nature and properties of the error term in regression equations and the interpretation of structural parameters in SEMs. The revolution established a new regime, which introduced new notation and a unified language, where causal and statistical concepts are finally separated. At last, correlation is never again to be confused with causation. “Introduction To Causality” is a gentle introduction into this modern understanding of causality as it unfolded after the revolution. It will help you learn the fundamentals of this art and science of causal analysis. After reading this book, you’ll have the tools to understand and communicate causal concepts and you will know how to tackle the common questions. The code in the appendix will help you to apply these methods using the R programming language. 1.1 What you will learn First we will discuss causality in a trivial lab environment where we are able to control every important aspect of the environment. This will help us get familiar with the vocabulary and notation and will provide some insights in how to think about causality. Once we have become familiar with the simple setting, we will loosen the assumption that we are able to fully control the environment. At this point, we will introduce causality as a probabilistic concept. Our inability to fully control and understand our environment forces us to settle for a less precise inference on the effects. We can’t say what will happen, but we can still provide robust inference on what will happen on average. Most importantly, we will see why classical statistical concepts and notation are not sufficient for a rigorous and unambiguous treatment of causality and we will get a sense that there is a fundamental difference between what can be learned from passive observation (“correlation”) and active intervention (“causation”). Once we mastered the probabilistic nature of causality we will discuss on how we can measure the effect of actions in a variety of settings. We will start with the easiest scenario, the randomized controlled trial. It is often considered the gold standard for clinical trials and applied across scientific disciplines. It will serve as a benchmark in our further discussion, where we will look at scenarios that violate the assumption behind the randomized controlled trial: we will discuss observational studies, synthetic cohorts and time series analysis. After this tour de force, we shift gears and have a closer look at a couple of applications. We will discuss how to measure and interpret the placebo effect in clinical trials, how to optimize marketing using A/B tests and multi-armed bandits, and how to evaluate government interventions. Finally, I will wrap up this book by providing some parting thoughts on epistemology and the importance of causality in the evolution of artificial intelligence and machine learning. These chapters will hopefully provide you with a solid foundation and will allow you to find the right solution for your causal problem. But for most of your problems they will not be enough. Throughout this book we’ll point you to resources where you can learn more. The appendix provides some additional material on statistics and programming. 1.2 How this book is organised 1.3 What you won’t learn There are some important topics that this book doesn’t cover. We hope that this book will leave you wanting more and that you will continue in your journey to master causality by going deeper into this topic or by exploring closly related fields and applications that we did not cover in sufficient length. 1.3.1 Statistics The book focuses on causal inference rather than statistics. Some basic statistical concepts are discussed in the appendix, but they primarily serve as a refresher. We assume that the reader is (or has been) familiar with statistics as it is taught in most Statistics 101 classes. Details on estimation methods and properties of estimators (e.g. consistency) are not discussed. We will provide references that provide more details. We will, however, extensively discuss the differences between these two types of inferences and how they relate. Our discussion on causal inference will, except for the basic introduction, be probabilistic in nature and statistical notation will be used throughout the book. We have summarized information on notation and terminology in CHAPTER XX. 1.3.2 Machine Learning We will address issues of machine learning where we see a connection to causal concepts. We do not go deep on any causal and non-causal ML algorithms. The discussion will focus on the discussion of supervised ML versus reinforcement learning. 1.3.3 Proofs The book does not contain any proof or any heavy mathematical derivations. We will link to reference material. Despite that, we do intend to be rigorous in argumentation and notation and some discussions might seem overly verbose at first. We believe however that this is necessary, especially to avoid confusion between statistical and causal concepts. 1.4 Prerequisites To get the most out of this book, you should be familiar with basic concepts of statistical analysis, nomeclature and notation. If “expected value”, “conditional probability” or “hypothesis test” are only vaguely familiar to you, please review the appendix before digging into the main text. The code snippets at the end of the book are purely optional. If you want to follow along on these, you need to have R on your computer. To download the software, go to CRAN, the comprehensive R archive n etwork. CRAN is composed of a set of mirror servers distributed around the world and is used to distribute R and R packages. Don’t try and pick a mirror that’s close to you: instead use the cloud mirror, https://cloud.r-project.org, which automatically figures it out for you. RStudio is an integrated development environment, or IDE, for R programming. Download and install it from http://www.rstudio.com/download. 1.5 Acknowledgements The book has been compiled from markdown documents using R package bookdown. This package has allowed me to adopt a very flexible workflow where the compilation and publication of an HTML version only takes seconds. 1.6 Links A free HTML version of this book is available at https://flobrez.github.io/itc/. The markdown sources and supplementary material is available at https://github.com/flobrez/itc. "],
["defining-causality.html", "2 Defining Causality 2.1 Causal Models 2.2 Causality in simple environments 2.3 Causality in complex environments 2.4 Measuring causal effects", " 2 Defining Causality 2.1 Causal Models We assume the world can be modelled by variables. Variables can take various values. The variables themselves are denoted by upper-case latin letters, e.g. \\(X\\), whereas we use lower-case letters for their values, e.g. \\(x\\). In case \\(X\\) is categorical, different values will be denoted by a subscript \\(x_j\\). Where \\(X\\) has two values only, we will encode them with \\(0\\) and \\(1\\). Structural equations represent the causal relations between variables. The absence of a variable from the model assumes that it is not relevant for the causal description of the system. We will focus exposition on categorical variables which can assume a \\(X \\rightarrow Y\\) means that \\(X\\) causes \\(Y\\). Manipulating \\(X\\) determines the value of \\(Y\\), but not the other way round. We call \\(X\\) the cause and \\(Y\\) the outcome. Others call \\(Y\\) the “effect”, but we will use effect to denote changes in the outcome due to manipulations of the cause. This is in line with conventions in statistical literature (e.g. “average treatment effect”) and its usage in everyday language (e.g. “tipping on that button had no effect on the brightness of the screen”). 2.2 Causality in simple environments Let’s first take a look at a maximally simple environment, shown in figure xxx. It represents a circuit diagram with a voltage source, a switch (X) and a lamp (Y). All elements of this environment can assume one of two states each, which we will conveniently encode as 0 and 1: * the switch can either be open (0) or closed (1) * the lamp can either be off (0) or on (1). Let’s further assume that the voltage source has enough capacity to lighten the lamp if the switch is closed. Although this system is easy to understand and reason about, let’s take an extra second to translate the circuit diagram into a causal graph, a representation that will become quite handy in more complex environments that will be discussed later in the book. Theorem 2.1 (Causal Graphs) A graph is a mathematical structure. It consists of a set of nodes and and a set of edges, where edges connect ordered pairs of nodes. In causal graphs, nodes represent variables; edges represent the causal relation from cause to effect. Note that in a causal graph, an edge is an ordered pair of nodes, the edge therefore directed. In most graphs in this book, we will consider causal systems that can be represeted as directed acyclical graphs (DAGs). We can further represent a causal graph as a set of structural equations. Due to its simplicity, the circuit diagram can be represented with a single equation: \\[\\begin{equation} Y := f(X) = X \\end{equation}\\] Note, that the equation uses operator “\\(:=\\)”\" rather than the usual “\\(=\\)”. It reads “\\(f(X)\\) is evaluated and assigned to \\(Y\\)” and therefore resembles variable assignment in many programming languages where, for example, x = x + 1 is a valid expression. Crucially, it is asymmetric: if \\(X\\) is the air temperature and \\(Y\\) the reading of the temperature on a thermometer, the reading will change if we heat up the air; manipulating the reading, e.g. by exposing the thermometer to direct sunlight will not heat up the air around it. Since the structural equation represents the causal mechanism relating the switch and the lamp, we can immediately read what happens if we intervene on the switch: when we close the switch, i.e. \\(do(X:= 1)\\), then the lamp will be on, \\(Y := 1\\); if we open the switch, \\(do(X:= 0)\\), then the lamp dies, \\(Y := 0\\). Let’s take it up a notch and create a more interesting environment by adding a second switch, connected in series, see figure XXX for the circuit diagram and figure xxx for the graph representation. The two switches allow the environment to be in four different states. Only if both switches are on, will the lamp be on, in the other three states it will be off: switch \\(X_1\\) switch \\(X_2\\) lamp \\(Y\\) 0 0 0 0 1 0 1 0 0 1 1 1 We can easily spot that the structural equation representation is \\[\\begin{equation} Y := f(X_1, X_2) = X_1 \\cdot X_2 \\end{equation}\\] Setzen wir in diesem System nun den Schalter 1 auf geschlossen, erkennen wir, dass der Zustand der Leuchte nun noch vom Zustand des zweiten Schalters abhängt. \\[\\begin{equation} Y := f(1, X_2) = 1 \\cdot X_2 = X_2 \\end{equation}\\] Nehmen wir nun an, dass wir nicht allwissend sind, sondern sich ein Schleier des Unwissens über Schalter 2 gelegt hat: wir wissen, dass es ihn gibt, wir wissen nur nicht, ob dieser geschlossen ist oder nicht. Man stellt uns nun die Aufgabe, den Zustand des Schalters 2 aus der rein passiven Beobachtung des Systems zu lernen. Ein Blick auf Tabelle xxx zeigt, dass uns das nur in einigen Fällen gelingen wird. Bestimmte Konstellationen von Schalter 1 und Leuchte geben uns nicht ausreichende Information, um den Zustand von Schalter 2 lernen zu können. Selbstverständlich könnten wir dieses Problem lösen, indem wir Schalter 1 betätigen und beobachten, wie sich Leuchte dann ändert. Tatsächlich können wir durch die Intervention und die weitere Beobachtung auf den Zustand von Schalter 2 zu schließen. Dies ist bereits ein erster Hinweis auf ein fundamentales Problem, auf das wir im Weiteren noch detaillierter eingehen werden: durch Interventionen in eine System können Informationen generiert werden, die aus rein passiver Beobachtung nicht verfügbar sind. Gehen wir nun noch einen Schritt weiter und überlegen wir uns, ob das Problem gelöst werden kann, indem wir 2.3 Causality in complex environments \\[\\begin{equation} \\Delta_i := Y_i^{S;do(Z_i:=1)} - Y_i^{S; do(Z_i:=0)} \\label{eq:myfirsteq} \\tag{1} \\end{equation}\\] As \\(Y\\) is binary, \\(\\Delta\\) can be one of \\({-1, 0, 1}\\) with \\(\\Delta = 1\\) being the desired outcome. As discussed in [causality], we are not able to measure this quantity directly, but need to resort to population-level quantities instead: \\[\\begin{equation} P(\\Delta) = P^{S;do(Z:=1)}(Y) - P^{S;do(Z:=0)}(Y) \\label{eq:mktg_pop_ate} \\tag{2} \\end{equation}\\] 2.4 Measuring causal effects The fundamental problem of causal inference The definition of [causal effect] hints at a severe problem for its measurements. It involves two quantities which can never be both observed at once. This poses a problem, a problem so fundamental that is often called the fundamental problem of causal inference. "],
["methods-for-causal-inference.html", "3 Methods for Causal Inference 3.1 Causal vs Statistical Inference 3.2 Randomized Controlled Experiments 3.3 Instrumental Variables 3.4 Propensity Score Matching 3.5 Difference-in-Difference Estimator 3.6 Time Series Methods", " 3 Methods for Causal Inference Causal relations can be inferred from experiments as well as observational studies. The randomized controlled experiment is a proven method to infer causal relations in complex environments. It involves full control over the assignment mechanism and the assignment is random. A common variation is the situation where the variable of interest cannot be directly intervened on, but a causal parent can. For example, a doctor can (randomly) prescribe a certain drug, but the patient still chooses to take the drug or not. The method of instrumental variables allows us to infer (local) causal effects nevertheless. Afterwards, we switch to those methods that can be used even if we cannot intervene in the environment, but have to rely on passive observation only. Inferring causal relations in these situations requires a thorough understanding of the causal links from the variable of interest to the effect. We will study two different inference strategies which rely on different sets of assumptions. Finally, we discuss methods for causal inference in samples of size 1. Given appropriate assumptions, we are able to infer causal relations by leveraging (dependent) observations over time. 3.1 Causal vs Statistical Inference Causal inference is much harder than statistical inference. 3.2 Randomized Controlled Experiments TODO 3.3 Instrumental Variables TODO 3.4 Propensity Score Matching TODO 3.5 Difference-in-Difference Estimator TODO 3.6 Time Series Methods TODO "],
["applications.html", "4 Applications 4.1 Marketing 4.2 Drug Trial 4.3 Discrimination", " 4 Applications 4.1 Marketing Another common application where correlation is often confused with causation is in marketing. This might be because data scientist and business might not speak the same language. It might, however, also be a valid pragmatic assumption, which is later validated in an experiment. I will focus here on the (mis-)application of propensity modeling. Propensity modeling attempts to predict if a (potential) customers will perform a certain action, e.g. whether a new visitor on your site will register or whether a customer will buy a certain product. The model output is an estimated probability that the customer will do the action. Propensity models therefore fall into the class of binary regression. \\[\\begin{equation} P(Y|I) = f(I) \\label{eq:mktg1} \\tag{1} \\end{equation}\\] where \\(I\\) is the information set available for prediction.1 There are plently of algorithms that could be used to estimate the function. Logistic regression is often chosen as it is easy to implement and the model itself might provide some insights. Here, we will focus not on the implementation part, but on the interpretation and (mis-) use of the model. To see why the model might not be want you think it is, we will have a A naive usage of the model is to focus marketing on customers with high likelhood to do the action. This however, can be serverly misleading, as we will discuss next. To show this, we will start with an ideal assumption: our model is perfect. A perfect model means that we predict the customer action correctly for every customer and the model produces predicted probabilies which are well calibrated. This means that we only get two predictions, either a customer will do action with estimated probability 0 or whether they will do so with probability 1 - and the model is always right. However, although we have the best possible model, it illustrates well why the naive interpreation cannot be correct. If we focus our marketing on those customers with highest propensity (as there are only two values it is those with predicted probability 1), we focus our attention on a customer group that buys the product anyways. In fact, these are the customers that we should least focus on as the best we can do is to have no effect on those customers (but we still have the cost of the marketing intervention, which might be an opportunity cost) and in some case we might even affect the customer adversely (as they might be annoyed by the marketing). Hence, we’re left with the second group of customers, those with predicted probability of 0. In this group, there might be some who will be convinced to buy the product after being exposed to the marketing, but we are not able to say which ones. Even after doing an experiment, we will not improve on our decision rule. Say, we run an A/B test on all customers who were predicted to not buy, and we estimate that 0.02 Depending on the situation, this model might not be very useful. The only thing that we learned from the model is that we should exclude those customers with highest probability from our marketing. This runs counter to our intuition. Furthermore, in many applications, the action might be a rare event, with likelihoods not much higher than 1%. Excluding these customers from marketing might not save a lot of money in the first place, and establishing a system where you’re able to provide different marketing on customer-level might have some fixed costs (e.g. it might require to store and process customer-level data and deploy the model-outputs to production systems). From the example it becomes clear that propensity modelling using a predictive model on passively observed data will at best be a proxy for the problem at hand. The goal of marketing optimization is to optimally trade-off the cost and effect of marketing, where the latter is a causal rather than an assosiative concept. Supervised machine learning models, regardless of their complexity, will fail at achieving the task since even a ideal falls short. To see this more clearly, let’s restate the problem in causal notation first. Let \\(Y\\) denote the binary customer action we want to predict. Further, let \\(S\\) denote the default environment, which includes all relevant causal factors that determine customer decisions, their preferences and endowment, the offers available on the market and our own offer and current marketing strategy. For simplicity’s sake, let’s assume our marketing strategy is binary and denote it by \\(M\\) (e.g. whether or not we send the customer a marketing email). Assume the current marketing strategy is \\(M = 0\\), i.e. we do currently not have an email program. The individual causal differential effect of sending an email to customer \\(i\\) is then \\[\\begin{equation} \\Delta_i := Y_i^{S;do(Z_i:=1)} - Y_i^{S; do(Z_i:=0)} \\label{eq:myfirsteq} \\tag{1} \\end{equation}\\] As \\(Y\\) is binary, \\(\\Delta\\) can be one of \\({-1, 0, 1}\\) with \\(\\Delta = 1\\) being the desired outcome. As discussed in [causality], we are not able to measure this quantity directly, but need to resort to population-level quantities instead: \\[\\begin{equation} P(\\Delta) = P^{S;do(Z:=1)}(Y) - P^{S;do(Z:=0)}(Y) \\label{eq:mktg_pop_ate} \\tag{2} \\end{equation}\\] Both quantities on the right-hand side of equation \\(\\eqref{eq:mktg_pop_ate}\\) can be estimated. There are couple of ways to do so, many of which we discussed in [causality]. The most straigtforward way is to apply a randomized controlled trial (or “A/B test”) where the population at hand is randomly split in two groups, one group being exposed to the marketing (i.e. \\(S;do(Z:=1)\\)) the other not being exposed (i.e. \\(S;do(Z:=0)\\)). Conditioning the probability estimate on a set of features allows us to investigate whether the differential causal effect is co-related with observable information - which ultimately tells us who will be most affected by the marketing. In the most simple case, we condition by a single discrete attribute \\(A\\), providing \\[\\begin{equation} P(\\Delta | A) = P^{S;do(Z:=1)}(Y | A) - P^{S;do(Z:=0)}(Y | A) \\label{eq:mktg_pop_cate} \\tag{3} \\end{equation}\\] This is superficially similar to equation \\(\\eqref{eq:mktg1}\\), but note that the right-hand side in $ describes two conditional probabilities drawn from two different environments. It will be helpful to rewrite this into a linear model form. Assume that \\(A\\) is binary. Then \\[\\begin{align} P(\\Delta | A = 1) &amp;= P^{S;do(Z:=1)}(Y | A = 1) - P^{S;do(Z:=0)}(Y | A = 1) \\\\ &amp;= a_1 - a_0 \\\\ &amp;=: \\Delta_1 \\\\ P(\\Delta | A = 0) &amp;= P^{S;do(Z:=1)}(Y | A = 0) - P^{S;do(Z:=0)}(Y | A = 0) \\\\ &amp;= a_3 - a_2 \\\\ &amp;=: \\Delta_0 \\end{align}\\] We can further represent \\(P(Y)\\) as a function of \\(Z\\) \\[\\begin{align} P(Y) &amp;= P^{S;do(Z:=0)}(Y) \\cdot (1-Z) + P^{S;do(Z:=1)}(Y) \\cdot Z \\\\ &amp;= P^{S;do(Z:=0)}(Y) + (P^{S;do(Z:=1)}(Y) - P^{S;do(Z:=0)}(Y)) \\cdot Z \\\\ &amp;= P^{S;do(Z:=0)}(Y) + P(\\Delta) \\cdot Z \\\\ &amp;= \\beta_0 + \\beta_1 Z \\end{align}\\] Further, replacing unconditional quantities with conditional ones, we can write \\[\\begin{align} P(Y | A) &amp;= P^{S;do(Z:=0)}(Y | A) + P(\\Delta | A) \\cdot Z \\\\ &amp;= \\alpha_0 + \\alpha_1 A + (\\Delta_0 + \\Delta_1 \\cdot A) \\cdot Z \\\\ &amp;= \\alpha_0 + \\alpha_1 A + \\Delta_0 Z + \\Delta_1 A Z \\end{align}\\] which looks quite familiar as it is the conventional way to specify a logistic regression equation on \\(A\\), \\(Z\\) and the interaction of both \\(A \\cdot Z\\).[^footnote-hte-nomenclature] If the treatment is ineffective \\(\\Delta_0 = 0\\) and \\(Delta_1 = 0\\). The differential causal effect is said to be heterogeneous, if \\(\\Delta_1 \\neq 0\\). [^footnote-hte-nomenclature]: The literature on heterogeneous treatment effect models often groups parameters of this equation regarding their role in application: \\(\\alpha_1\\) is called “prognostic” as it shows if and how the success rate differs across attributes \\(A\\) if no intervention/treatment is provided; \\(\\Delta_1\\) on the other hand is often called “predictive”, meaning how predictive \\(A\\) is on the effectiveness of the intervention/treatment, i.e. whether and by how much treatment effects differ across values of \\(A\\). The model can be generalized to the case where not just a single (binary) attribute is considered, but a vector of attributes. In a marketing context with binary treatment and outcome, table xxx can be restated as Table 4.1: Caption here \\(Y^{\\Gamma;do(X:=0)} = 0\\) \\(Y^{\\Gamma;do(X:=0)} = 1\\) \\(Y^{\\Gamma;do(X:=1)} = 0\\) “Lost Cause” “Do-Not-Disturb” \\(Y^{\\Gamma;do(X:=1)} = 1\\) “Persuadable” “Sure Things” 4.2 Drug Trial 4.3 Discrimination In den letzten Jahren wurde in Politik, Medien und Wissenschaft häufiger über Diskriminierung diskutiert, einige Beispiele sind hierbei die Diskriminierung von Frauen im Berufsleben, der Wahl von Abegordneten zum Bundestag. Ein Verbot von Diskriminierung hat in Deutschland im Jahre 2006 Gesetzescharakter angenommen mit der Verabschiedung des Allgemeinen Gleichbehandlungsgesetzes (AGG), dass u.a. die Diskriminierung aus Gründen des Alters, der ethnischen Herkunft oder des Geschlechts unzulässig ist. Wie bereits des Gesetzesname andeutet, wird Diskriminierung als ein Vorgang angesehen, nicht als ein Zustand. Dies wird insebesondere in §3 deutlich, wo eine (unmittelbare) Diskriminierung dann vorliegt “wenn eine Person wegen [Alter, Herkunft oder Geschlecht] eine weniger günstige Behandlung erfährt, als eine andere Person in einer vergleichbaren Situation erfährt”. Die zunehmende Anwendung automatisierter Entscheidungsalgorithmen rückt die Frage nach Diskrimierung auch in den Blickpunkt der Forschung im Bereich des maschinellen Lernens. Es ist in diesem Kontext, dass dieses Thema eine mathematische Formalisierung erfahren hat. Dabei wurden zuletzt auch kausale Argumentationen und Notation eingeführt, siehe etwa Kilbertus etl al.. Wir werden uns im Folgenden an einem einfachen Beispiel diesem Problem nähern sowie einen Versuch der Die Identifikation von diskriminierenden Handlungen wird dadurch erschwert, dass die Handelnden Personen häufig nur einen Teil des gesamten Mechanismus kontrollieren und es begründete Sachzwänge gibt. Im Folgenden unterscheiden wir zwei Merkmalstypen First, let’s introduce some basic terminology first: * protected feature: this is a person’s feature that is legally protected, e.g. age or gender. * accepted feature: these are features that are explicitely or implicitely accepted for discrimination, e.g. the job might (by law) require a certain certificate of qualification or the job requires a certain skillset like a specific programming language or the ability to lift weigths heavier than 30 kg. While the protected features are usually unambiguous and explicitly stated in law, the accepted features can be reason for disagreement. It is usually the set of features that are deemed to be necessary to do the job, but of course “doing the job” can done in different qualities. A bar owner might believe it to be necessary for his waiters to be handsome and flirty with the predominantely female audience, but an applicant might think it is not. Getting the order right and providing the right servcie while being friendly he might consider to be sufficient. unprotected feature: these are all features that are not explicitly protected, e.g. eye color, ability to lift more than 30kg zulässige Merkmale: dies sind Merkmale, nach denen eine diskriminierende Handlung zulässig ist. So kann etwa für eine Tätigkeit in einem Warenlager voraussgesetzt werden, dass ein Bewerber in der Lage sein muss, Lasten über 20kg zu transportieren, da dies für die Erfüllung der Tätigkeit unabdingbar ist. aufhebende Merkmale: diese stellen eine Teilmenge der zulässigen Merkmale dar. Es sind diejenigen Merkmale, die auf dem kausalen Pfad zwischen den geschützten Merkmalen und dem Output liegen. Das Merkmal “kann Lasten über 20kg transportieren” ist vermutlich ein solches, da dieses kausal vom Geschlecht und/oder dem Alter eines Bewerbers beeinflusst ist. unzulässige Merkmale: sind Merkmale, nach denen eine diskriminierende Handlung nicht zulässig ist, die aber nicht ein geschütztes Merkmal sind. Dies könnte etwa die Augenfarbe des Bewerbers sein, die (etwa gemäß AGG nicht geschützt ist), jedoch für die Erfüllung der Tätigkeit irrelevant sein dürfte. stellvertretende Merkmale: diese stellen eine Teilmenge der unzulässigen Merkmale dar. Es sind diejenige Merkmale, die auf dem kausalen Pfad zwischen den geschützten Merkmalen und dem Output liegen. So kann die Augenfarbe eines Bewerbers ein stellvertretendes Merkmal sein, wenn die Augenfarbe etwa durch das Geschlecht oder die ethnische Herkunft beeinflusst wird. Sie werden stellvertretend genannt, da sie Information über das geschützte Merkmals beinhalten und somit zur Diskriminierung herangezogen werden könnten. TODO: sollte unzulässig nicht anders bezeichnet werden, da ja eine Diskriminierung nach nicht-stellvertretern rechtlich nicht verboten ist. Es gelten folgende Definitionen: kann ein Mechanismus der unmittelbaren Diskriminierung durch einen vom geschützten Merkmal unabhängigen Prozess (oder eine Konstante) ersetzt werden, ohne dass sich die Verteilung der Zielvariable ändert, so ist dieser Mechanismus nicht-diskriminierend Typically you will find a notation similar to \\(P(Y|X) = f(X)\\) where \\(X\\) is called a feature set or a vector/matrix of features. I use the term information set to deliberately distinguish between the notion of all the information you have available for a customer. The information set is abstract and represents information in potentially very different formats, e.g. order data in your data base, the recorded call with the customer service team, or the customer’s product reviews. Feature engineering is the step where this information is transformed into a format that can be used by ML algorithms: the order data could be transformed into multiple features, e.g. the total revenue in past 30 days, total revenue in past 180 days, the number of days the customer put an order in; the recorded call can transformed into days since the latest customer service contanct, length of the call, length of waiting in line and whether or not the issue was resolved; the product reviews are transformed into word embeddings.↩︎ "],
["epistemology.html", "5 Epistemology 5.1 Manipulation 5.2 Long-Term Effects 5.3 Are all sciences equal?", " 5 Epistemology 5.1 Manipulation Sometimes the cause cannot be directly manipulated: the doctor can prescribe a drug but not ensure the patient is taking the drug; a marketing manager might choose who to sent a mail to, but the delivery could fail; we dealt with problems like these in our application on instrumental variables. The problem is however, more subtle, and probably more important than we might have anticipated. The issue appears in situations where cannot directly intervene on a variable, but only on a mechanism that affects that variable. Assume, you’re an economist asked to evaluate the effect of university education on a persons lifetime income to inform a decision on whether or not to expand access to university education. As an expert in economics, statistics and causal inference, you immediately recognize the problems with this task. * knowing about treatment effect heterogeneity, you understand that the average treatment effect calculated for the current group of students is not an estimate for the treatment effect for those who will become available under the new policy. * you’re asked to provide a causal effect, but the intervention is not specified. The government can’t directly manipulate Uncertainty about effectiveness of interventions can have various reasons: * correct model: we are uncertain about the assumptions on the causal structure used to make the causal inference * extrapolation: the causal inference relied on data drawn from other populations, either other individual or a different time. In dynamic environments such as the economy, causal knowledge can depreciate fast with new technology, a changing competition and ever-so slightly different regulation. * In many circumstances the optimal decision will differ by perspective. Imagine a doctor having two different treatments at hand. Drug A is known to be most effective (100), but relies on the patient taking the drug reliably every day. If taken irregularly, the drug’s effectiveness is severly lower (50). Drug B on the other hand works similarly if take regularly or irregularly (75). If the doctor has to make a decision whether to prefer A vs B, she will have to rely on her judgements of the patient’s ability to follow through on a regular schedule. If it is known that most patients have a hard time doing this, the best treatment to prescribe from a doctor’s perspective is drug B. The patient, however, might think differently. Knowing that they have the self-discipline they might choose to go with drug A. This is just one example on how a guideline with simple recommendations can be sub-optimal. If the patient was given the information, he could use it to make a better judgement (but of course the patient might be overconfident in his ability to stick to a rigid schedule). This is similar to giving someone advice to exercise and restrict to 2000kcal food intake to loose weight. This certainly works if one adhere’s to it. The likelihood to fail will be great though. A less ambitious and rigid goal might be less effective, but might be substantially easier to follow-through. The recommendation for a diet will have to have a larger effect than the more rigid one. 5.2 Long-Term Effects In examples so far we have considered situations where the intervention was one-shot, i.e. we only intervened once in the system. Although plausible in the applications considered, there are many applications where this is not the case. Especially when we are interested in long-term goals rather than short-term outcomes, it is implausible that we would not be able to intervene in the future. Of course, this can partly be alleviated estimating the likelihood of future actions and taken these into account, but that doesn’t make sense if we are the actors ourselves. The British xx was tasked to evaluate the effect of Brexit on British GDP in the years after Brexit. This is difficult to establish as many important decisions are undetermined by Brexit itself (leaving out the problem that the terms and conditions of the Brexit are not fully specified themselves). Leaving the EU will allow Great Britain to explore policies that were not available within the EU: every country in the EU has to have a value-added tax system. Leaving the EU allows to choose a different policy. An economist trying to provide this estimate will have to assume a probability distribution on this topic, as it will certainly have non-trivial impact. A politician in charge, e.g. the prime minister, might disregard this recommendation as he might have quite different plans and the probability distribution might be quite different (it might not be fully centered at a single option as he might be unsure whether he might achieve this policy if he is willing to go for it). 5.3 Are all sciences equal? The concept of causality as discussed in this book is a cornerstone of many scientific disciplines. Although in most cases not stated explicitly in those terms, the question what will happen if i does x (versus y) is implicit in most scientific work in diverse fields such as economics, psychology, medicine, and history. The interventions contemplated and analyzed in these disciplines vary widely in terms of manipulability, scope and ethics: a randomized experiment on fiscal policy is neither operationally feasible nor ethically desirable - at least at a the level of entire nations; assessing how individuals provide different answers if questions are phrased differently is both manipulable in a controlled environment and too unintrusive to be ethically problematic. Throughout the book we discussed several methods to infer causal relationships ranging from randomized controlled trials to time series analysis. With every step away from the ideal properties of an RCT, the reliability and robustness of results crucially depended on the validity of the assumptions that we were willing to accept. In general, the more assumptions we make, the more likely the results will be unreliable. On a sidenote: of course, incorrect causal assumptions are not the only or even the main reason for scientific results to be unreliable. Many of those have to do with problems of statisical inference instead. Even in experiments following an RCT design, there are plenty of pitfalls to be aware of. Too small samples, p-hacking, and publishing bias are just a few reasons for the replication crisis in psychology, medicine and other social sciences. Least reliable results are to be expected when the complexity of the subject matter is very high, but the methodoligal toolkit to investigate it is reduced to the least reliable ones macroeconomics. growth, business cycle, monetary and fiscal policy. epidemiology. including nutrition. social psychology. evolution. while the theory of evolution has strong empirical support, combinatorial explosion and complex dynamics do not allow for a reliable inference of effects of interventions in the biosphere. Even if complexity is high, if some aspects of the problem can be thoroughly studied and combined using a consistent theory, the overall results can be reasonably reliable: climate science. microeconomics. pseudo-experiments on price controls (e.g. minimum wage or gas price cap) supported by strong theory of supply and demand. problem is often quantification of effects rather than the general direction. There is almost no disagreement across labor economists that there exists a general trade-off between higher minimum wages and lower unemployment. The debate today primarily focuses on the possibility of small negative or even positive effects on employment, but virtually everybody accepts the logic that unemployment will exist/increase above a certain threshold. "],
["notation-and-terminology.html", "A Notation and Terminology", " A Notation and Terminology Throughout the book we will follow notation and terminology similar to Peters, Janzik, and Schölkopf (2017). It deviates from Pearl (2009) primarily in the notation for intervention distributions. Where Pearl (2009) writes \\(P(Y|do(X = x))\\), we will use \\(P^{\\Gamma;do(X:=x)}(Y)\\) instead: it avoids confusion with the notation for conditional distributions and it emphasizes that an intervention is an assignment (“\\(:=\\)”) rather than an equality (“\\(=\\)”). Otherwise we follow conventional statistical notation. symbol represents \\(X, Y, U\\) (random) variable \\(x\\) value of \\(X\\) \\(X =x\\) \\(X\\) has value \\(x\\) \\(X:=x\\) assignment of value \\(x\\) to variable \\(X\\) \\(i\\) sample index, observation \\(\\Gamma\\) causal graph \\(\\Gamma;do(X:=x)\\) intervention on graph \\(\\Gamma\\) by assigning value \\(x\\) to \\(X\\) \\(P(Y)\\) probability distribution of \\(Y\\) \\(P(Y|X = x)\\) probability distribution of \\(Y\\) given \\(X = x\\) \\(P(Y|X)\\) set of probability distributions \\(P(Y|X = x) \\forall x\\) \\(P^{\\Gamma;do(X:=x)}(Y)\\) probability distributions of \\(Y\\) after intervention on graph \\(\\Gamma\\) \\(E(Y), \\mu_Y\\) expected value of \\(Y\\) \\(Var(Y), \\sigma_Y^2\\) variance of \\(Y\\) \\(\\hat{\\beta}\\) estimate of parameter \\(\\beta\\) \\(X -&gt; Y\\) causal link from \\(X\\) to \\(Y\\) References "],
["statistics-1.html", "B Statistics B.1 Distributions B.2 Statistical Inference", " B Statistics This section provides a brief introduction into concepts of probability theory and statistical inference that are essential for understanding the technical parts of the main text. B.1 Distributions B.1.1 Single Random Variable The expected value of a discrete random variable \\(X\\) is the weighted avarage of the possible values \\(x\\), with their probabilites as weights \\[\\begin{equation} E(X) = \\sum_{x} p(x) x \\end{equation}\\] The expected value of a sum of two random variables is the sum of their expected values \\[\\begin{equation} E(X + Y) = E(X) + E(Y) \\end{equation}\\] and the expected value scales linearly with scaling factor \\(a\\) \\[\\begin{equation} E(a X) = a E(X) \\end{equation}\\] Note, however, that this property does not hold for the product of two random variables \\[\\begin{equation} E(X \\cdot Y) = E(X) \\cdot E(Y) \\end{equation}\\] only if \\(X\\) and \\(Y\\) are independent. B.1.2 Multiple Random Variables Covariance Correlation Conditional Probability Regression Parameter B.2 Statistical Inference B.2.1 Estimators B.2.2 Hypothesis Tests "],
["code.html", "C Code C.1 Equivalence of statistical properties and SQL", " C Code C.1 Equivalence of statistical properties and SQL The probability distribution \\(P(X = x)\\) SELECT X AS value_x , SUM(1.0) / SUM(SUM(1.0)) OVER () AS probability_of_x FROM table GROUP BY X The expected value \\(E(X)\\) SELECT AVG(X) AS expected_value_of_X FROM table The set of conditional expected values \\(E(X|Y)\\) SELECT Y AS value_y , AVG(X) AS cond_expected_value_X_given_y FROM table GROUP BY Y The conditional expected values \\(E(X|Y = y)\\) SELECT Y AS value_y , AVG(X) AS cond_expected_value_X_given_y FROM table WHERE Y = y GROUP BY Y "]
]
