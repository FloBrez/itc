<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Causal Models | Introduction To Causality: A Modern Approach</title>
  <meta name="description" content="A gentle introduction into the art and science of causal inference" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Causal Models | Introduction To Causality: A Modern Approach" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://flobrez.github.io/intro_to_causality/" />
  
  <meta property="og:description" content="A gentle introduction into the art and science of causal inference" />
  <meta name="github-repo" content="flobrez/itc" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Causal Models | Introduction To Causality: A Modern Approach" />
  
  <meta name="twitter:description" content="A gentle introduction into the art and science of causal inference" />
  

<meta name="author" content="Florian Brezina" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="methods-for-causal-inference.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="itc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Introduction To Causality</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-you-will-learn"><i class="fa fa-check"></i><b>1.1</b> What you will learn</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-you-wont-learn"><i class="fa fa-check"></i><b>1.2</b> What you won’t learn</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#statistics"><i class="fa fa-check"></i><b>1.2.1</b> Statistics</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#machine-learning"><i class="fa fa-check"></i><b>1.2.2</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#proofs"><i class="fa fa-check"></i><b>1.2.3</b> Proofs</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#type-causality-vs-actual-causality"><i class="fa fa-check"></i><b>1.2.4</b> Type Causality vs Actual Causality</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#how-this-book-is-organised"><i class="fa fa-check"></i><b>1.3</b> How this book is organised</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#prerequisites"><i class="fa fa-check"></i><b>1.4</b> Prerequisites</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#acknowledgements"><i class="fa fa-check"></i><b>1.5</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#links"><i class="fa fa-check"></i><b>1.6</b> Links</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="causal-models.html"><a href="causal-models.html"><i class="fa fa-check"></i><b>2</b> Causal Models</a><ul>
<li class="chapter" data-level="2.1" data-path="causal-models.html"><a href="causal-models.html#causality-asymmetry-and-entropy"><i class="fa fa-check"></i><b>2.1</b> Causality, Asymmetry and Entropy</a></li>
<li class="chapter" data-level="2.2" data-path="causal-models.html"><a href="causal-models.html#basic-definitions"><i class="fa fa-check"></i><b>2.2</b> Basic Definitions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="causal-models.html"><a href="causal-models.html#causal-graphs"><i class="fa fa-check"></i><b>2.2.1</b> Causal Graphs</a></li>
<li class="chapter" data-level="2.2.2" data-path="causal-models.html"><a href="causal-models.html#structural-equations"><i class="fa fa-check"></i><b>2.2.2</b> Structural Equations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="causal-models.html"><a href="causal-models.html#simple-environments"><i class="fa fa-check"></i><b>2.3</b> Simple Environments</a><ul>
<li class="chapter" data-level="2.3.1" data-path="causal-models.html"><a href="causal-models.html#an-electric-circuit-with-one-switch"><i class="fa fa-check"></i><b>2.3.1</b> An electric circuit with one switch</a></li>
<li class="chapter" data-level="2.3.2" data-path="causal-models.html"><a href="causal-models.html#an-electric-circuit-with-two-switches"><i class="fa fa-check"></i><b>2.3.2</b> An electric circuit with two switches</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="causal-models.html"><a href="causal-models.html#unobservability"><i class="fa fa-check"></i><b>2.4</b> Unobservability</a><ul>
<li class="chapter" data-level="2.4.1" data-path="causal-models.html"><a href="causal-models.html#probabilistic-models-of-causality"><i class="fa fa-check"></i><b>2.4.1</b> Probabilistic Models of Causality</a></li>
<li class="chapter" data-level="2.4.2" data-path="causal-models.html"><a href="causal-models.html#interventions-more-generally-defined"><i class="fa fa-check"></i><b>2.4.2</b> Interventions More Generally Defined</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="causal-models.html"><a href="causal-models.html#complex-environments"><i class="fa fa-check"></i><b>2.5</b> Complex Environments</a><ul>
<li class="chapter" data-level="2.5.1" data-path="causal-models.html"><a href="causal-models.html#an-electric-circuit-with-two-switches-one-unobserved"><i class="fa fa-check"></i><b>2.5.1</b> An electric circuit with two switches, one unobserved</a></li>
<li class="chapter" data-level="2.5.2" data-path="causal-models.html"><a href="causal-models.html#section"><i class="fa fa-check"></i><b>2.5.2</b> </a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="causal-models.html"><a href="causal-models.html#causal-effects"><i class="fa fa-check"></i><b>2.6</b> Causal Effects</a><ul>
<li class="chapter" data-level="2.6.1" data-path="causal-models.html"><a href="causal-models.html#definition"><i class="fa fa-check"></i><b>2.6.1</b> Definition</a></li>
<li class="chapter" data-level="2.6.2" data-path="causal-models.html"><a href="causal-models.html#causal-effect-statistics"><i class="fa fa-check"></i><b>2.6.2</b> Causal Effect Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html"><i class="fa fa-check"></i><b>3</b> Methods for Causal Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#causal-vs-statistical-inference"><i class="fa fa-check"></i><b>3.1</b> Causal vs Statistical Inference</a><ul>
<li class="chapter" data-level="3.1.1" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#example-1-tutoring"><i class="fa fa-check"></i><b>3.1.1</b> Example 1 Tutoring</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#randomized-controlled-experiments"><i class="fa fa-check"></i><b>3.2</b> Randomized Controlled Experiments</a><ul>
<li class="chapter" data-level="3.2.1" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#assignment-mechanisms"><i class="fa fa-check"></i><b>3.2.1</b> Assignment Mechanisms</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#observational-data"><i class="fa fa-check"></i><b>3.3</b> Observational Data</a></li>
<li class="chapter" data-level="3.4" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#instrumental-variables"><i class="fa fa-check"></i><b>3.4</b> Instrumental Variables</a></li>
<li class="chapter" data-level="3.5" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#propensity-score-matching"><i class="fa fa-check"></i><b>3.5</b> Propensity Score Matching</a></li>
<li class="chapter" data-level="3.6" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#difference-in-difference-estimator"><i class="fa fa-check"></i><b>3.6</b> Difference-in-Difference Estimator</a></li>
<li class="chapter" data-level="3.7" data-path="methods-for-causal-inference.html"><a href="methods-for-causal-inference.html#time-series-methods"><i class="fa fa-check"></i><b>3.7</b> Time Series Methods</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>4</b> Applications</a><ul>
<li class="chapter" data-level="4.1" data-path="applications.html"><a href="applications.html#causality-and-machine-learning"><i class="fa fa-check"></i><b>4.1</b> Causality and Machine Learning</a></li>
<li class="chapter" data-level="4.2" data-path="applications.html"><a href="applications.html#marketing"><i class="fa fa-check"></i><b>4.2</b> Marketing</a></li>
<li class="chapter" data-level="4.3" data-path="applications.html"><a href="applications.html#drug-trial"><i class="fa fa-check"></i><b>4.3</b> Drug Trial</a></li>
<li class="chapter" data-level="4.4" data-path="applications.html"><a href="applications.html#discrimination"><i class="fa fa-check"></i><b>4.4</b> Discrimination</a></li>
<li class="chapter" data-level="4.5" data-path="applications.html"><a href="applications.html#epidemiology"><i class="fa fa-check"></i><b>4.5</b> Epidemiology</a><ul>
<li class="chapter" data-level="4.5.1" data-path="applications.html"><a href="applications.html#the-reporting-mechanism"><i class="fa fa-check"></i><b>4.5.1</b> The reporting mechanism</a></li>
<li class="chapter" data-level="4.5.2" data-path="applications.html"><a href="applications.html#evolving-symptoms"><i class="fa fa-check"></i><b>4.5.2</b> Evolving Symptoms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="epistemology.html"><a href="epistemology.html"><i class="fa fa-check"></i><b>5</b> Epistemology</a><ul>
<li class="chapter" data-level="5.1" data-path="epistemology.html"><a href="epistemology.html#manipulation"><i class="fa fa-check"></i><b>5.1</b> Manipulation</a></li>
<li class="chapter" data-level="5.2" data-path="epistemology.html"><a href="epistemology.html#long-term-effects"><i class="fa fa-check"></i><b>5.2</b> Long-Term Effects</a></li>
<li class="chapter" data-level="5.3" data-path="epistemology.html"><a href="epistemology.html#are-all-sciences-equal"><i class="fa fa-check"></i><b>5.3</b> Are all sciences equal?</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notation-and-terminology.html"><a href="notation-and-terminology.html"><i class="fa fa-check"></i><b>A</b> Notation and Terminology</a><ul>
<li class="chapter" data-level="A.1" data-path="notation-and-terminology.html"><a href="notation-and-terminology.html#terminology-confusion"><i class="fa fa-check"></i><b>A.1</b> Terminology Confusion</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="statistics-1.html"><a href="statistics-1.html"><i class="fa fa-check"></i><b>B</b> Statistics</a><ul>
<li class="chapter" data-level="B.1" data-path="statistics-1.html"><a href="statistics-1.html#distributions"><i class="fa fa-check"></i><b>B.1</b> Distributions</a><ul>
<li class="chapter" data-level="B.1.1" data-path="statistics-1.html"><a href="statistics-1.html#single-random-variable"><i class="fa fa-check"></i><b>B.1.1</b> Single Random Variable</a></li>
<li class="chapter" data-level="B.1.2" data-path="statistics-1.html"><a href="statistics-1.html#multiple-random-variables"><i class="fa fa-check"></i><b>B.1.2</b> Multiple Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="statistics-1.html"><a href="statistics-1.html#statistical-inference"><i class="fa fa-check"></i><b>B.2</b> Statistical Inference</a><ul>
<li class="chapter" data-level="B.2.1" data-path="statistics-1.html"><a href="statistics-1.html#estimators"><i class="fa fa-check"></i><b>B.2.1</b> Estimators</a></li>
<li class="chapter" data-level="B.2.2" data-path="statistics-1.html"><a href="statistics-1.html#hypothesis-tests"><i class="fa fa-check"></i><b>B.2.2</b> Hypothesis Tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="code.html"><a href="code.html"><i class="fa fa-check"></i><b>C</b> Code</a><ul>
<li class="chapter" data-level="C.1" data-path="code.html"><a href="code.html#equivalence-of-statistical-properties-and-sql"><i class="fa fa-check"></i><b>C.1</b> Equivalence of statistical properties and SQL</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction To Causality: A Modern Approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="causal-models" class="section level1">
<h1><span class="header-section-number">2</span> Causal Models</h1>
<div id="causality-asymmetry-and-entropy" class="section level2">
<h2><span class="header-section-number">2.1</span> Causality, Asymmetry and Entropy</h2>
<p><em>Causality</em> is strongly linked to the concept of <em>time</em>. Cause precedes effect, never the other way round. Symptoms occur after infection
This asymmetry is mirrored in the physical notion of entropy and (as an emergent property) time.
While the past is determined we feel that we are able to act on the future, that we are able to choose one among many possible futures. This is due to the low entropy the universe had in the past.
All fundamental physical laws are perfectly symmetrical and therefore reversable. Asymmetry is only introduced by a coarse-grained look at the world, and therefore an <em>emergent</em> property.</p>
</div>
<div id="basic-definitions" class="section level2">
<h2><span class="header-section-number">2.2</span> Basic Definitions</h2>
<p>We assume the world can be modelled by <em>variables</em>. Variables can take various values. The variables themselves are denoted by upper-case latin letters, e.g. <span class="math inline">\(X\)</span>, whereas we use lower-case letters for their values, e.g. <span class="math inline">\(x\)</span>. In case <span class="math inline">\(X\)</span> is <em>categorical</em>, different values will be denoted by a subscript <span class="math inline">\(x_j\)</span>. Where <span class="math inline">\(X\)</span> has two values only, we will encode them with <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
<div id="causal-graphs" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Causal Graphs</h3>

<div class="definition">
<span id="def:graphs" class="definition"><strong>Definition 2.1  (Causal Graph)  </strong></span>A graph is a mathematical structure. It consists of a set of nodes and and a set of edges, where edges connect ordered pairs of nodes. In <em>causal graphs</em>, nodes represent variables; edges represent the causal relation from cause to effect. Note that in a causal graph, an edge is an <em>ordered</em> pair of nodes, the edge therefore directed. In most graphs in this book, we will consider causal systems that can be represeted as directed acyclical graphs (DAGs)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. These DAGs have no feedback loops.
</div>

<p>The causal graphs convey the qualitative pattern of causal relations. They do not quantify that relation, i.e. specify how two variables are related. A graph with relation <span class="math inline">\(A \rightarrow B\)</span>
It The quantitative aspects are better represented in a set of structural equations.</p>

<div class="definition">
<span id="def:exoendo" class="definition"><strong>Definition 2.2  (Exogeneous and Endogeneous Variables)  </strong></span>An exogeneous variable in a graph G has no edges pointing into itself.
An endogeneous variable in a graph G has at least one edge point into itself.
</div>

</div>
<div id="structural-equations" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Structural Equations</h3>
<p><em>Structural equations</em> represent the causal relations between <em>variables</em>. The <em>absence</em> of a variable from the model assumes that it is not relevant for the causal description of the system.
We will focus exposition on <em>categorical variables</em> which can assume a</p>
<p><span class="math inline">\(X \rightarrow Y\)</span> means that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>. Manipulating <span class="math inline">\(X\)</span> determines the value of <span class="math inline">\(Y\)</span>, but not the other way round. We call <span class="math inline">\(X\)</span> the <em>cause</em> and <span class="math inline">\(Y\)</span> the <em>outcome</em>. Others call <span class="math inline">\(Y\)</span> the “<em>effect</em>”, but we will use <em>effect</em> to denote changes in the outcome due to manipulations of the cause. This is in line with conventions in statistical literature (e.g. “average treatment effect”) and its usage in everyday language (e.g. “tipping on that button had no effect on the brightness of the screen”).</p>
</div>
</div>
<div id="simple-environments" class="section level2">
<h2><span class="header-section-number">2.3</span> Simple Environments</h2>
<div id="an-electric-circuit-with-one-switch" class="section level3">
<h3><span class="header-section-number">2.3.1</span> An electric circuit with one switch</h3>
<p>Let’s first take a look at a most simple environment, shown in figure xxx. It represents a circuit diagram with a voltage source, a switch (X) and a lamp (Y). Both, X and Y, can assume one of two states. We will encode these as 0 and 1:</p>
<ul>
<li>switch is open (0) or closed (1)</li>
<li>light bulb is off (0) or on (1).</li>
</ul>
<div class="figure">
<img src="images/causal_models-simple_electric_1.png" alt="" />
<p class="caption">A simple circuit diagram with a single power source, a switch (X) and a light bulb (Y)</p>
</div>
<p>This system is very easy to reason about. Assuming that the power source has enough capacity, the light bulb will be on if and only if the switch is closed. The system can be in one of only two states:</p>
<table>
<thead>
<tr class="header">
<th>switch <span class="math inline">\(X\)</span></th>
<th>light bulb <span class="math inline">\(Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>This table, however, does not contain any information on the causal relationship between X and Y and will therefore not be sufficient to correctly reason about the system. Hence, let’s take an extra second to translate this circuit diagram into a <em>causal graph</em>. This will become much more useful in more complex systems, but it’s probably a good idea to get used to this representation from the start.</p>
<div class="figure">
<img src="images/causal_models-dag_simple_environment.png" alt="" />
<p class="caption">The simple circuit diagram converted to a causal graph, where X is the (only) cause of Y</p>
</div>
<p>A <em>causal graph</em> represents variables as <em>nodes</em> and causal relationships as <em>directed edges</em> between nodes. <span class="math inline">\(X \rightarrow Y\)</span> means that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>. This in turn means that manipulating <span class="math inline">\(X\)</span> determines the state of <span class="math inline">\(Y\)</span>, but not the other way round. Here, the graph consists of two nodes, the switch X and the light bulb Y, connected by a directed edge from X to Y.</p>
<p>While the graphical representation of the causal structure makes it easy to <em>qualitatively</em> reason about the causal structure of the system, an algebraic representation will be needed for quantitative analysis. The algebraic representation of a causal relationship is called a <em>structural equation</em>. In any system we have one structural equation for every node that has at least a cause.
In our case, only the light bulb Y has a cause and therefore we only have a single equation:
<span class="math display">\[\begin{equation}
Y := f(X)
\end{equation}\]</span>
where <span class="math inline">\(f()\)</span> is a function.
Note, that a <em>structural equation</em> uses “<span class="math inline">\(:=\)</span>” rather than the usual “<span class="math inline">\(=\)</span>”. It should be read as “<span class="math inline">\(f(X)\)</span> is evaluated and <em>assigned</em> to <span class="math inline">\(Y\)</span>”. It resembles variable assignment in many programming languages where, for example, <code>x = x + 1</code> is a valid expression. Here too, the current value of <code>x</code> is incremented by 1 and then reassigned to <code>x</code>. Crucially, a <em>structural equation</em> is asymmetric, capturing the important distinction that X causes Y, but not the other way around.</p>
<p>In many applications, the goal is to identify <span class="math inline">\(f()\)</span>. Here, however, our understanding of physics and the assumptions made about the system, allows us to infer <span class="math inline">\(f()\)</span> right away:
<span class="math display">\[\begin{equation}
Y := f(X) = X
\end{equation}\]</span></p>
<div id="intervention" class="section level4">
<h4><span class="header-section-number">2.3.1.1</span> Intervention</h4>
<p>Now that we managed to represent this system in various forms, we can start to reason about <em>interventions</em>. For now, we define an <em>intervention</em> as an operation in the system that fixes a variable to a certain value. Here, two interventions are of interest: we can open the switch, i.e. set <span class="math inline">\(X:=0\)</span>, or close it, i.e. set <span class="math inline">\(X:=1\)</span>. The <em>structural equation</em> is then
<span class="math display">\[\begin{equation}
Y := f(X) = 0
\end{equation}\]</span>
and
<span class="math display">\[\begin{equation}
Y := f(X) = 1
\end{equation}\]</span>
respectively.</p>
<p>This simple system has a nice property: for the light bulb to be on, the closed switch is a <em>necessary</em> and <em>sufficient</em> condition. This property is “nice” as it allows us to falsify the causal model from observation: a <em>single</em> observation where the light bulb is on but the switch is open (or the light bulb is off but the switch is closed) allows us to refute the causal model, e.g. the power source might not be strong enough or the circuit might have flaws.</p>
</div>
</div>
<div id="an-electric-circuit-with-two-switches" class="section level3">
<h3><span class="header-section-number">2.3.2</span> An electric circuit with two switches</h3>
<p>Let us now slightly increase the complexity of the system by adding a second switch. Both switches are connected in series as shown in the circuit diagram in figure XXX</p>
<div class="figure">
<img src="images/causal_models-simple_electric_2.png" alt="" />
<p class="caption">A circuit diagram with a single power source, two switches (X1 and X2) and a light bulb (Y)</p>
</div>
<p>Again, our understanding of the physical nature of this system allow us to derive the corresponding causal graph. Both switches cause the state of the light bulb whereas the light bulb does not cause any of the switches and switch <span class="math inline">\(X_1\)</span> does not cause switch <span class="math inline">\(X_2\)</span>, nor vice versa. Hence our causal graph has three nodes and directed edges <span class="math inline">\(X_1 \rightarrow Y\)</span> and <span class="math inline">\(X_2 \rightarrow Y\)</span>:</p>
<div class="figure">
<img src="images/causal_models-dag_simple_electric_2.png" alt="" />
<p class="caption">The simple circuit diagram converted to a causal graph, where X is the (only) cause of Y</p>
</div>
<p>The structural equation for this system is also very simple:
<span class="math display">\[\begin{equation}
Y := f(X_1, X_2)
\end{equation}\]</span>
but the functional form of <span class="math inline">\(f()\)</span> might not be obvious. The two switches allow the environment to be in four different states. Only if <em>both</em> switches are on, will the light bulb be on, in the other three states it will be off:</p>
<table>
<thead>
<tr class="header">
<th>switch <span class="math inline">\(X_1\)</span></th>
<th>switch <span class="math inline">\(X_2\)</span></th>
<th>light bulb <span class="math inline">\(Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Hence, the structural equation has to be
<span class="math display">\[\begin{equation}
Y := f(X_1, X_2) = X_1 \cdot X_2
\end{equation}\]</span></p>
<p>For the light bulb to be on, <span class="math inline">\(X_1=1\)</span> and <span class="math inline">\(X_2=1\)</span> are necessary conditions but neither is (on its own) sufficient.</p>
<div id="intervention-1" class="section level4">
<h4><span class="header-section-number">2.3.2.1</span> Intervention</h4>
<p>Let us now turn to interventions in this system. While in the system with just one switch, every intervention on <span class="math inline">\(X\)</span> caused a <em>change</em> in the state of <span class="math inline">\(Y\)</span>, this is not true in the case of interventions on a single switch now. In a state where <span class="math inline">\(X_1 = 0\)</span>, we are unable to change the state of <span class="math inline">\(Y\)</span> by intervening on <span class="math inline">\(X_2\)</span>:
<span class="math display">\[\begin{equation}
Y := f(0, X_2) = 0 \cdot X_2 = 0
\end{equation}\]</span></p>
<p>Conversely, if the first switch is closed, <span class="math inline">\(X_1 = 1\)</span>, we’re basically back in a system with just one switch, which solely determines by the state of the light bulb.
<span class="math display">\[\begin{equation}
Y := f(1, X_2) = 1 \cdot X_2 = X_2
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="unobservability" class="section level2">
<h2><span class="header-section-number">2.4</span> Unobservability</h2>
<p>So far, we were able to reason about the effectiveness of interventions due to our ability to fully specify the structural equations (i.e. we knew <span class="math inline">\(f()\)</span>) <em>and</em> were able to <em>observe</em> the state of all causes. This allowed us to reason that closing switch 2 will <em>not</em> have an effect on the light bulb when switch 1 is open, but will change its state if switch 1 is closed.</p>
<p>Problems arise, when one of these conditions fails. Suppose we are still dealing with the system with two switches connected in series, but the state of switch 1 cannot be observed as the switch is hidden in a box, see figure XXX.</p>
<div class="figure">
<img src="images/causal_models-hidden_electric_2.png" alt="" />
<p class="caption">A circuit diagram with a single power source, two switches (X1 and X2) and a light bulb (Y), but switch 1 is hidden in a box.</p>
</div>
<p>Imagine now that we observe that switch 2 is open (and the light bulb is off). Will closing switch 2 turn on the light?</p>
<p>Unfortunately, we are not any more able to answer this question. Our best answer is “It depends.”. It depends on the state of switch 1, which is not observable. What were left is to resort to a different kind of reasoning, <em>probabilistic</em> reasoning. While we’re not able to make any statements about any single system of that kind, we can still make statments on the likelihood of <span class="math inline">\(X_2\)</span> effectiveness based on probability distributions for <span class="math inline">\(X_1\)</span>. If we knew that the likelihood that switch 1 is closed is 0.8 in all cases where we encounter switch 2 to be open and and the light to be off, then closing switch 2 will turn on the light in 80% of the cases.</p>
<div id="probabilistic-models-of-causality" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Probabilistic Models of Causality</h3>
<p>This example has shown that even in very simple causal systems, not being able to observe (or accurately measure) a single variable requires us to revert to inferences of a lesser kind, <em>probabilistic</em> rather than <em>determinstic</em>. Of course, outside of physics, most systems worth studying are far more complex as the one described here, and many variables of (potential) interest cannot be observed or measured. Causal reasoning in social sciences, medicine and other complex sciences is therefore closely linked with probability theory. Hence, from here on, we will consider these probabilistic use cases.</p>
</div>
<div id="interventions-more-generally-defined" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Interventions More Generally Defined</h3>
<p>So far, we have used the term <em>intervention</em> when we fixed a variable of system to a certain value. More generally, an intervention is a change in one or more structural equations. An intervention that replaces the right-hand side of the structural equation with a <em>value</em> is called <em>hard intervention</em> (or <em>atomic</em>, <em>ideal</em>, <em>surgical</em>), whereas a replacement with another function with the same arguments is called a <em>soft intervention</em> (or <em>imperfect</em>, <em>parametric</em>) (<span class="citation">(Pearl <a href="#ref-pearl2009" role="doc-biblioref">2009</a>, 35)</span> and <span class="citation">(Pearl <a href="#ref-pearl2009" role="doc-biblioref">2009</a>, 89)</span>)</p>
<p>We already encountered examples for hard interventions, like closing switch <span class="math inline">\(X_1\)</span>:
<span class="math display">\[\begin{equation}
X_1 := 1
\end{equation}\]</span>.</p>
<p>A soft intervention would be a rearrangment of the circuit, so <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are placed are placed in parallel rather than in series. The state of the light bulb would still depend on both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, but the assignment functions would be more like this:
<span class="math display">\[\begin{equation}
Y := g(X_1, X_2) = I(X_1 + X_2 &gt;= 0)
\end{equation}\]</span>
where <span class="math inline">\(I(a)\)</span> is the indicator function that returns 1 if <span class="math inline">\(a\)</span> is true and 0 otherwise. In a parallel arrangement it is sufficient that <em>any</em> of the two switches is closed:</p>
<table>
<thead>
<tr class="header">
<th>switch <span class="math inline">\(X_1\)</span></th>
<th>switch <span class="math inline">\(X_2\)</span></th>
<th>light bulb <span class="math inline">\(Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="complex-environments" class="section level2">
<h2><span class="header-section-number">2.5</span> Complex Environments</h2>
<div id="an-electric-circuit-with-two-switches-one-unobserved" class="section level3">
<h3><span class="header-section-number">2.5.1</span> An electric circuit with two switches, one unobserved</h3>
<p>Let us start with the problem above, where we model a system with two switches, but the state of switch one is unobserved. To make the distinction between the two switches more apparent, let’s denote the unobserved switch by <span class="math inline">\(U\)</span> and the observable switch simply by <span class="math inline">\(X\)</span>:</p>
<div class="figure">
<img src="images/causal_models-hidden_electric_ux.png" alt="" />
<p class="caption">A circuit diagram with a single power source, a two switches (X and U) and a light bulb (Y). Switch U is unobserved.</p>
</div>
<p>Note that the <em>structure</em> of the causal graphs remains the same, as the box only hinders us to observe the <em>state</em> of the switch, but we still know that there is a switch. This allows us to further specify <span class="math inline">\(f()\)</span> of the structural equation, which after refactoring variable names is now
<span class="math display">\[\begin{equation}
Y := f(U, X) = X \cdot U
\end{equation}\]</span></p>
<p>Let us further introduce a symbol for the system itself, <span class="math inline">\(S\)</span>, which not only denotes the structure of the system, but includes the probabilistic mechanism that determines <span class="math inline">\(U\)</span>. We can now <em>derive</em> the <em>probability distributions</em> for <span class="math inline">\(Y\)</span> from the structural equation for each possible interventions on <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\begin{equation}
P^{S;do(X:=x)}(Y=y) = P^{S;do(X:=x)}(X \cdot U = y) = P^{S;do(X:=x)}(x \cdot U) = x P^{S;do(X:=x)}(U)
\end{equation}\]</span>
where <span class="math inline">\(do(X:=x)\)</span> denotes the intervention on <span class="math inline">\(X\)</span> where its state is set to <span class="math inline">\(x\)</span>. Note that, as the intervention fixes <span class="math inline">\(X\)</span> to a specific value, it is non-random and can therefore be placed outside of <span class="math inline">\(P()\)</span>.</p>
<p>Causal reasoning about the effect of the switch on the light bulb (the effect of setting the state of <span class="math inline">\(X\)</span> on the state of <span class="math inline">\(Y\)</span>) now depends not on the state of <span class="math inline">\(U\)</span>, but its probability distribution. We can safely say that, if we open the switch, then the light bulb will never be on, regardless of the probability distribution of <span class="math inline">\(U\)</span>:
<span class="math display">\[\begin{equation}
P^{S;do(X:=0)}(Y=1) = 0 \cdot P^{S;do(X:=0)}(U) = 0
\end{equation}\]</span>
Again, if we were to observe a case where the switch is open but the light bulb is on, we would instantly know that our structural equation is invalid.</p>
<p>This is different however for the case where we close the switch:
<span class="math display">\[\begin{equation}
P^{S;do(X:=1)}(Y=1) = 1 \cdot P^{S;do(X:=1)}(U=1) = P^{S}(U=1)
\end{equation}\]</span></p>
<p>Say, for example, system <span class="math inline">\(S\)</span> refers to a population of circuits that are all created by the same machine. This machine is supposed to close the switch in the box, but fails to do so 1% of the cases. Hence, <span class="math inline">\(P^{S}(U=1)=0.99\)</span> and therefore <span class="math inline">\(P^{S;do(X:=1)}(Y=1)=0.99\)</span>: only in 99% of the cases where we close the switch, we will see the light bulb go on, in 1% of the cases it will fail to do so.</p>
</div>
<div id="section" class="section level3">
<h3><span class="header-section-number">2.5.2</span> </h3>
<p><span class="math display">\[\begin{equation}
\Delta_i := Y_i^{S;do(Z_i:=1)} - Y_i^{S; do(Z_i:=0)} \label{eq:myfirsteq} \tag{1}
\end{equation}\]</span></p>
<p>As <span class="math inline">\(Y\)</span> is binary, <span class="math inline">\(\Delta\)</span> can be one of <span class="math inline">\({-1, 0, 1}\)</span> with <span class="math inline">\(\Delta = 1\)</span> being the desired outcome. As discussed in [causality], we are not able to measure this quantity directly, but need to resort to population-level quantities instead:
<span class="math display">\[\begin{equation}
P(\Delta) = P^{S;do(Z:=1)}(Y) - P^{S;do(Z:=0)}(Y) \label{eq:mktg_pop_ate} \tag{2}
\end{equation}\]</span></p>
</div>
</div>
<div id="causal-effects" class="section level2">
<h2><span class="header-section-number">2.6</span> Causal Effects</h2>
<p>The fundamental problem of causal inference</p>
<p>The definition of [causal effect] hints at a severe problem for its measurements. It involves two quantities which can never be observed at once. This poses a severe problem, often called <strong>the fundamental problem of causal inference</strong>.
Nevertheless, it does not prevent us from inferring <em>average</em> causal effects. This might be counterintuitive at first. How could we measure the <em>average</em> of a quantity, if we can’t measure the quantity itself? We will see that statistics comes to the rescue. The linearity of expectation states that</p>
<p><span class="math display">\[\begin{equation}
E(U - V) = E(U) - (V)
\end{equation}\]</span>
i.e. expected value of the difference of two random variables is the difference between the expected values of the individual random variables. Hence, even if <span class="math inline">\(U - V\)</span> cannot be observed, we can still calculate.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<div id="definition" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Definition</h3>
<p>bla</p>
</div>
<div id="causal-effect-statistics" class="section level3">
<h3><span class="header-section-number">2.6.2</span> Causal Effect Statistics</h3>

<div class="definition">
<span id="def:ate" class="definition"><strong>Definition 2.3  (Average Treatment Effect)  </strong></span>The Average Treatment Effect, or ATE, is the expected value of xx in population x.
</div>

<p>bla bla bla</p>

<div class="definition">
<span id="def:att" class="definition"><strong>Definition 2.4  (Average Treatment Effect on the Treated)  </strong></span>The Average Treatment Effect on the Treated, or ATT, is the expected value of xx in population x conditional on observing x.
</div>

<p>It is often used in situations where the treatment effect is expected to be heterogeneous in a population. In a given environment, selection into treatment could yield treated individuals to have a different average treatment effect than the total population. For example, if university eduction has a higher effect on earnings for people with high intelligence and if people with high intelligence more often chose a university education than less intelligent ones, the average effect of university eduction of those who choose to go to university will be higher than in the overall population (and therefore than those choosing not to go to university).</p>

<div class="definition">
<span id="def:itt" class="definition"><strong>Definition 2.5  (Intention To Treat Effect)  </strong></span>The Intention To Treat Effect, or ITT, is the expected value of xx in population x.
</div>

<p>It is conceptually the same as the ATE, but often refers to a situation where the primary intervention cannot be manipulated directly, e.g. where a doctor can prescribe a drug but not enforce that the patient actually takes the drug.</p>

<div class="definition">
<span id="def:late" class="definition"><strong>Definition 2.6  (Local Average Treatment Effect)  </strong></span>The Average Treatment Effect, or ATE, is the expected value of xx in population x.
</div>


</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-pearl2009">
<p>Pearl, Judea. 2009. <em>Causality</em>. 2nd ed. Cambridge University Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Readers familiar with DAGs data processing pipelines will recognize that these too describe causal mechanisms. Datasets are manipulated in an ordered sequence of steps to produce a final outcome where the result of each step is determined by the outcome of its parents steps (the input datasets) and the mechanism itself (the transformation of the datasets).<a href="causal-models.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Imagine you are interested in the average <em>net income</em> of a certain population, i.e. <span class="math inline">\(E(income - expenses)\)</span>. Even if you do not have access to individual-level data, say due to privacy concerns, you can calculate this value if you are given the population averages of income and expenses, i.e. <span class="math inline">\(E(income) - E(expenses)\)</span>. Note that linearity is a property of the expected value, but not of other aggregate metris that might be of interest like the median value, where, in general, <span class="math inline">\(Median(income - expenses) \neq Median(income) - Median(expenses)\)</span>.<a href="causal-models.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methods-for-causal-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/flobrez/itc/edit/master/causality.md",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
